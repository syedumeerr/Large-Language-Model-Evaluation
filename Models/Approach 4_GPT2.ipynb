{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":7696767,"sourceType":"datasetVersion","datasetId":4492320}],"dockerImageVersionId":30646,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n","metadata":{"execution":{"iopub.status.busy":"2024-02-25T13:33:14.539473Z","iopub.execute_input":"2024-02-25T13:33:14.540534Z","iopub.status.idle":"2024-02-25T13:33:14.909930Z","shell.execute_reply.started":"2024-02-25T13:33:14.540483Z","shell.execute_reply":"2024-02-25T13:33:14.909079Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"/kaggle/input/test-train-imdb/train.csv\n/kaggle/input/test-train-imdb/test.csv\n","output_type":"stream"}]},{"cell_type":"code","source":"from IPython.display import clear_output\n# !pip install wandb\nclear_output()","metadata":{"execution":{"iopub.status.busy":"2024-02-25T13:33:14.911304Z","iopub.execute_input":"2024-02-25T13:33:14.911635Z","iopub.status.idle":"2024-02-25T13:33:14.917042Z","shell.execute_reply.started":"2024-02-25T13:33:14.911612Z","shell.execute_reply":"2024-02-25T13:33:14.916160Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"import os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","metadata":{"execution":{"iopub.status.busy":"2024-02-25T13:33:14.918126Z","iopub.execute_input":"2024-02-25T13:33:14.918459Z","iopub.status.idle":"2024-02-25T13:33:14.928876Z","shell.execute_reply.started":"2024-02-25T13:33:14.918435Z","shell.execute_reply":"2024-02-25T13:33:14.928035Z"},"trusted":true},"execution_count":3,"outputs":[{"name":"stdout","text":"/kaggle/input/test-train-imdb/train.csv\n/kaggle/input/test-train-imdb/test.csv\n","output_type":"stream"}]},{"cell_type":"code","source":"!pip install datasets\n!pip install peft\n!pip install bs4\nclear_output()","metadata":{"execution":{"iopub.status.busy":"2024-02-25T13:33:14.931166Z","iopub.execute_input":"2024-02-25T13:33:14.931493Z","iopub.status.idle":"2024-02-25T13:33:52.743950Z","shell.execute_reply.started":"2024-02-25T13:33:14.931463Z","shell.execute_reply":"2024-02-25T13:33:52.742857Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"\nimport pandas as pd\nimport datasets\nfrom transformers import RobertaTokenizerFast, RobertaForSequenceClassification,Trainer, TrainingArguments\nimport torch.nn as nn\nimport torch\nfrom torch.utils.data import Dataset, DataLoader\nimport numpy as np\nfrom sklearn.metrics import accuracy_score, precision_recall_fscore_support\nfrom tqdm import tqdm\n# import wandb\nimport os\nimport re\n\nfrom transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig,HfArgumentParser,TrainingArguments,pipeline, logging\nfrom peft import LoraConfig, PeftModel, prepare_model_for_kbit_training, get_peft_model\nimport os,torch\nfrom datasets import load_dataset\n# from trl import SFTTrainer\n\nfrom torch.utils.data import DataLoader\nfrom transformers import DistilBertForSequenceClassification, AdamW\n\n# import spacy\n# import pandas as pd\n# import numpy as np\n# import nltk\n# from nltk.tokenize.toktok import ToktokTokenizer\n# import re\n# from bs4 import BeautifulSoup\n# from contractions import CONTRACTION_MAP\n# import unicodedata\n\n\n# import pandas as pd\n# from sklearn.feature_extraction.text import TfidfVectorizer\n# from sklearn.model_selection import train_test_split\n# from sklearn.naive_bayes import MultinomialNB\n# from sklearn.ensemble import RandomForestClassifier\n# from sklearn.neighbors import KNeighborsClassifier\n# from sklearn.metrics import accuracy_score\n# from nltk.tokenize import word_tokenize\n# from nltk.corpus import stopwords\n# from nltk.stem import WordNetLemmatizer\n# from nltk import pos_tag\n# import string\n\n# tokenizer = ToktokTokenizer()\n\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\ndevice","metadata":{"execution":{"iopub.status.busy":"2024-02-25T13:33:52.745516Z","iopub.execute_input":"2024-02-25T13:33:52.746206Z","iopub.status.idle":"2024-02-25T13:34:00.593193Z","shell.execute_reply.started":"2024-02-25T13:33:52.746164Z","shell.execute_reply":"2024-02-25T13:34:00.592285Z"},"trusted":true},"execution_count":5,"outputs":[{"name":"stderr","text":"2024-02-25 13:33:57.691490: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n2024-02-25 13:33:57.691544: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n2024-02-25 13:33:57.693103: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"},{"execution_count":5,"output_type":"execute_result","data":{"text/plain":"device(type='cuda')"},"metadata":{}}]},{"cell_type":"code","source":"from bs4 import BeautifulSoup\n\ntrain_dataset = pd.read_csv('/kaggle/input/test-train-imdb/train.csv')\ntest_dataset = pd.read_csv('/kaggle/input/test-train-imdb/test.csv')\n\ntrain_dataset['label'] = [1 if x==\"positive\" else 0 for x in train_dataset['sentiment'] ]\ntest_dataset['label'] = [1 if x==\"positive\" else 0 for x in test_dataset['sentiment'] ]\n\ntrain_dataset = train_dataset.drop('sentiment', axis=1)\ntest_dataset = test_dataset.drop('sentiment', axis=1)\n\ndef strip_html_tags(text):\n    soup = BeautifulSoup(text, \"html.parser\")\n    stripped_text = soup.get_text()\n    return stripped_text\n\n\n\ntest_dataset['review'] = test_dataset['review'].apply(strip_html_tags)\ntrain_dataset['review'] = train_dataset['review'].apply(strip_html_tags)\n\n\n\ntrain_texts, train_labels = train_dataset['review'], train_dataset['label']\ntest_texts, test_labels = test_dataset['review'], test_dataset['label']\n\ntrain_texts.reset_index(drop=True, inplace=True)\ntest_texts.reset_index(drop=True, inplace=True)\n\nfrom sklearn.model_selection import train_test_split\n\ntrain_texts, val_texts, train_labels, val_labels = train_test_split(train_texts, train_labels, test_size=.2)\n\ntrain_texts.reset_index(drop=True, inplace=True)\nval_texts.reset_index(drop=True, inplace=True)\ntrain_labels.reset_index(drop=True, inplace=True)\nval_labels.reset_index(drop=True, inplace=True)\n\nfrom transformers import GPT2Tokenizer\n","metadata":{"execution":{"iopub.status.busy":"2024-02-25T13:34:00.594545Z","iopub.execute_input":"2024-02-25T13:34:00.595312Z","iopub.status.idle":"2024-02-25T13:34:10.642730Z","shell.execute_reply.started":"2024-02-25T13:34:00.595274Z","shell.execute_reply":"2024-02-25T13:34:10.641692Z"},"trusted":true},"execution_count":6,"outputs":[{"name":"stderr","text":"/tmp/ipykernel_74/1840652599.py:13: MarkupResemblesLocatorWarning: The input looks more like a filename than markup. You may want to open this file and pass the filehandle into Beautiful Soup.\n  soup = BeautifulSoup(text, \"html.parser\")\n","output_type":"stream"}]},{"cell_type":"code","source":"from transformers import (\n    RobertaForSequenceClassification,\n    RobertaTokenizer,\n    RobertaModel,\n)\n\nmodel_name = \"gpt2\"\ntokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\", do_lower_case=True)\ntokenizer.pad_token = tokenizer.eos_token\ntrain_encodings = tokenizer(list(train_texts), truncation=True, padding=True)\ntest_encodings = tokenizer(list(test_texts), truncation=True, padding=True)\nval_encodings = tokenizer(list(val_texts), truncation=True, padding=True)\n\nimport torch\n\nclass IMDBdataset(torch.utils.data.Dataset):\n    def __init__(self, encodings, labels):\n        self.encodings = encodings\n        self.labels = labels\n\n        # Ensure all encodings and labels have consistent lengths\n        assert all(len(val) == len(self.labels) for val in self.encodings.values()), \"Encodings and labels lengths do not match.\"\n\n    def __len__(self):\n        return len(self.labels)\n\n    def __getitem__(self, idx):\n        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n        item['labels'] = torch.tensor(self.labels[idx])\n        return item\n\ntrain_dataset = IMDBdataset(train_encodings, train_labels)\ntest_dataset = IMDBdataset(test_encodings,test_labels)\nval_dataset = IMDBdataset(val_encodings, val_labels)\n\nfrom transformers import AutoTokenizer, GPT2ForSequenceClassification\n\nmodel = GPT2ForSequenceClassification.from_pretrained(\n        model_name, num_labels=2, output_attentions=False, output_hidden_states=False)\n\nif torch.cuda.device_count() > 1:\n    print(f\"Using {torch.cuda.device_count()} GPUs!\")\n    model = torch.nn.DataParallel(model)\n    \nmodel.to(device)","metadata":{"execution":{"iopub.status.busy":"2024-02-25T13:34:10.644079Z","iopub.execute_input":"2024-02-25T13:34:10.644903Z","iopub.status.idle":"2024-02-25T13:36:21.436115Z","shell.execute_reply.started":"2024-02-25T13:34:10.644864Z","shell.execute_reply":"2024-02-25T13:36:21.435216Z"},"trusted":true},"execution_count":7,"outputs":[{"name":"stderr","text":"Some weights of GPT2ForSequenceClassification were not initialized from the model checkpoint at gpt2 and are newly initialized: ['score.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"},{"name":"stdout","text":"Using 2 GPUs!\n","output_type":"stream"},{"execution_count":7,"output_type":"execute_result","data":{"text/plain":"DataParallel(\n  (module): GPT2ForSequenceClassification(\n    (transformer): GPT2Model(\n      (wte): Embedding(50257, 768)\n      (wpe): Embedding(1024, 768)\n      (drop): Dropout(p=0.1, inplace=False)\n      (h): ModuleList(\n        (0-11): 12 x GPT2Block(\n          (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n          (attn): GPT2Attention(\n            (c_attn): Conv1D()\n            (c_proj): Conv1D()\n            (attn_dropout): Dropout(p=0.1, inplace=False)\n            (resid_dropout): Dropout(p=0.1, inplace=False)\n          )\n          (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n          (mlp): GPT2MLP(\n            (c_fc): Conv1D()\n            (c_proj): Conv1D()\n            (act): NewGELUActivation()\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n      )\n      (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n    )\n    (score): Linear(in_features=768, out_features=2, bias=False)\n  )\n)"},"metadata":{}}]},{"cell_type":"code","source":"from transformers import GPT2ForSequenceClassification\n\nmodel = GPT2ForSequenceClassification.from_pretrained(\"gpt2\", num_labels=2)","metadata":{"execution":{"iopub.status.busy":"2024-02-25T13:36:21.437341Z","iopub.execute_input":"2024-02-25T13:36:21.437639Z","iopub.status.idle":"2024-02-25T13:36:21.945750Z","shell.execute_reply.started":"2024-02-25T13:36:21.437614Z","shell.execute_reply":"2024-02-25T13:36:21.944984Z"},"trusted":true},"execution_count":8,"outputs":[{"name":"stderr","text":"Some weights of GPT2ForSequenceClassification were not initialized from the model checkpoint at gpt2 and are newly initialized: ['score.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"}]},{"cell_type":"code","source":"from transformers import Trainer, TrainingArguments\n\ntraining_args = TrainingArguments(\n    output_dir=\"./results\",  # directory where the results and model checkpoints will be saved\n    learning_rate=5e-5,  # learning rate for the optimizer\n    per_device_train_batch_size=1,  # batch size per device during training\n    num_train_epochs=3,  # number of training epochs\n    weight_decay=0.01,  # weight decay for regularization\n    fp16=True,\n    save_steps=2000  # change save_steps from 500 to 2000\n)\n\ntrainer = Trainer(\n    model=model,  # the instantiated 🤗 Transformers model to be trained\n    args=training_args,  # training arguments, defined above\n    train_dataset=train_dataset,  # training dataset\n    eval_dataset=val_dataset,  # evaluation dataset\n)\n\ntrainer.train()\n","metadata":{"execution":{"iopub.status.busy":"2024-02-25T13:39:25.803502Z","iopub.execute_input":"2024-02-25T13:39:25.804177Z","iopub.status.idle":"2024-02-25T14:25:15.601661Z","shell.execute_reply.started":"2024-02-25T13:39:25.804130Z","shell.execute_reply":"2024-02-25T14:25:15.599862Z"},"trusted":true},"execution_count":10,"outputs":[{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='6765' max='36000' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [ 6765/36000 45:47 < 3:17:58, 2.46 it/s, Epoch 0.56/3]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Step</th>\n      <th>Training Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>500</td>\n      <td>2.010300</td>\n    </tr>\n    <tr>\n      <td>1000</td>\n      <td>2.231800</td>\n    </tr>\n    <tr>\n      <td>1500</td>\n      <td>2.178200</td>\n    </tr>\n    <tr>\n      <td>2000</td>\n      <td>1.979000</td>\n    </tr>\n    <tr>\n      <td>2500</td>\n      <td>2.271200</td>\n    </tr>\n    <tr>\n      <td>3000</td>\n      <td>2.045100</td>\n    </tr>\n    <tr>\n      <td>3500</td>\n      <td>1.647300</td>\n    </tr>\n    <tr>\n      <td>4000</td>\n      <td>1.988000</td>\n    </tr>\n    <tr>\n      <td>4500</td>\n      <td>1.813000</td>\n    </tr>\n    <tr>\n      <td>5000</td>\n      <td>2.017100</td>\n    </tr>\n    <tr>\n      <td>5500</td>\n      <td>2.180900</td>\n    </tr>\n    <tr>\n      <td>6000</td>\n      <td>1.850400</td>\n    </tr>\n    <tr>\n      <td>6500</td>\n      <td>1.842500</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","Cell \u001b[0;32mIn[10], line 20\u001b[0m\n\u001b[1;32m      3\u001b[0m training_args \u001b[38;5;241m=\u001b[39m TrainingArguments(\n\u001b[1;32m      4\u001b[0m     output_dir\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m./results\u001b[39m\u001b[38;5;124m\"\u001b[39m,  \u001b[38;5;66;03m# directory where the results and model checkpoints will be saved\u001b[39;00m\n\u001b[1;32m      5\u001b[0m     learning_rate\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m5e-5\u001b[39m,  \u001b[38;5;66;03m# learning rate for the optimizer\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     10\u001b[0m     save_steps\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2000\u001b[39m  \u001b[38;5;66;03m# change save_steps from 500 to 2000\u001b[39;00m\n\u001b[1;32m     11\u001b[0m )\n\u001b[1;32m     13\u001b[0m trainer \u001b[38;5;241m=\u001b[39m Trainer(\n\u001b[1;32m     14\u001b[0m     model\u001b[38;5;241m=\u001b[39mmodel,  \u001b[38;5;66;03m# the instantiated 🤗 Transformers model to be trained\u001b[39;00m\n\u001b[1;32m     15\u001b[0m     args\u001b[38;5;241m=\u001b[39mtraining_args,  \u001b[38;5;66;03m# training arguments, defined above\u001b[39;00m\n\u001b[1;32m     16\u001b[0m     train_dataset\u001b[38;5;241m=\u001b[39mtrain_dataset,  \u001b[38;5;66;03m# training dataset\u001b[39;00m\n\u001b[1;32m     17\u001b[0m     eval_dataset\u001b[38;5;241m=\u001b[39mval_dataset,  \u001b[38;5;66;03m# evaluation dataset\u001b[39;00m\n\u001b[1;32m     18\u001b[0m )\n\u001b[0;32m---> 20\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/transformers/trainer.py:1539\u001b[0m, in \u001b[0;36mTrainer.train\u001b[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[1;32m   1537\u001b[0m         hf_hub_utils\u001b[38;5;241m.\u001b[39menable_progress_bars()\n\u001b[1;32m   1538\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1539\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minner_training_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1540\u001b[0m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1541\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1542\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrial\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1543\u001b[0m \u001b[43m        \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1544\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/transformers/trainer.py:1874\u001b[0m, in \u001b[0;36mTrainer._inner_training_loop\u001b[0;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[1;32m   1868\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maccelerator\u001b[38;5;241m.\u001b[39maccumulate(model):\n\u001b[1;32m   1869\u001b[0m     tr_loss_step \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining_step(model, inputs)\n\u001b[1;32m   1871\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m   1872\u001b[0m     args\u001b[38;5;241m.\u001b[39mlogging_nan_inf_filter\n\u001b[1;32m   1873\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_torch_tpu_available()\n\u001b[0;32m-> 1874\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m (torch\u001b[38;5;241m.\u001b[39misnan(tr_loss_step) \u001b[38;5;129;01mor\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43misinf\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtr_loss_step\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m   1875\u001b[0m ):\n\u001b[1;32m   1876\u001b[0m     \u001b[38;5;66;03m# if loss is nan or inf simply add the average of previous logged losses\u001b[39;00m\n\u001b[1;32m   1877\u001b[0m     tr_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m tr_loss \u001b[38;5;241m/\u001b[39m (\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mglobal_step \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_globalstep_last_logged)\n\u001b[1;32m   1878\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "],"ename":"KeyboardInterrupt","evalue":"","output_type":"error"}]},{"cell_type":"code","source":"from transformers import GPT2Tokenizer, GPT2ForSequenceClassification\n\n\ntokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\", do_lower_case=True)\nmodel = GPT2ForSequenceClassification.from_pretrained(\"/kaggle/working/results/checkpoint-6000\", return_dict=False)\nmodel.to(device)\n\n# Ensure your model is in evaluation mode\nmodel.eval()","metadata":{"execution":{"iopub.status.busy":"2024-02-25T14:25:20.862482Z","iopub.execute_input":"2024-02-25T14:25:20.863307Z","iopub.status.idle":"2024-02-25T14:25:21.588536Z","shell.execute_reply.started":"2024-02-25T14:25:20.863273Z","shell.execute_reply":"2024-02-25T14:25:21.587504Z"},"trusted":true},"execution_count":12,"outputs":[{"execution_count":12,"output_type":"execute_result","data":{"text/plain":"GPT2ForSequenceClassification(\n  (transformer): GPT2Model(\n    (wte): Embedding(50257, 768)\n    (wpe): Embedding(1024, 768)\n    (drop): Dropout(p=0.1, inplace=False)\n    (h): ModuleList(\n      (0-11): 12 x GPT2Block(\n        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n        (attn): GPT2Attention(\n          (c_attn): Conv1D()\n          (c_proj): Conv1D()\n          (attn_dropout): Dropout(p=0.1, inplace=False)\n          (resid_dropout): Dropout(p=0.1, inplace=False)\n        )\n        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n        (mlp): GPT2MLP(\n          (c_fc): Conv1D()\n          (c_proj): Conv1D()\n          (act): NewGELUActivation()\n          (dropout): Dropout(p=0.1, inplace=False)\n        )\n      )\n    )\n    (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n  )\n  (score): Linear(in_features=768, out_features=2, bias=False)\n)"},"metadata":{}}]},{"cell_type":"code","source":"train_dataloader = DataLoader(dataset=train_dataset, batch_size=16, shuffle=True)\nval_dataloader = DataLoader(dataset=val_dataset, batch_size=16, shuffle=False)\ntest_dataloader = DataLoader(dataset=test_dataset, batch_size=1, shuffle=False)","metadata":{"execution":{"iopub.status.busy":"2024-02-25T14:30:17.708644Z","iopub.execute_input":"2024-02-25T14:30:17.708971Z","iopub.status.idle":"2024-02-25T14:30:17.715892Z","shell.execute_reply.started":"2024-02-25T14:30:17.708948Z","shell.execute_reply":"2024-02-25T14:30:17.714849Z"},"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"code","source":"from sklearn.metrics import accuracy_score\nfrom tqdm import tqdm\nmodel.eval()  # Ensure the model is in evaluation mode\n\n# Store predictions and actual labels\npredictions = []\nactuals = []\n\nwith torch.no_grad():\n    for batch in tqdm(test_dataloader):\n        # Move tensors to the same device as the model\n        input_ids = batch['input_ids'].to(device)\n        attention_mask = batch['attention_mask'].to(device)\n        labels = batch['labels'].to(device)  # Only needed if you're also evaluating performance\n\n        outputs = model(input_ids, attention_mask=attention_mask)\n\n        # Assuming you're doing classification and want the highest probability class\n        logits = outputs[0]\n        predicted_labels = torch.argmax(logits, dim=1)\n        predictions.extend(predicted_labels.cpu().numpy())","metadata":{"execution":{"iopub.status.busy":"2024-02-25T14:30:42.683967Z","iopub.execute_input":"2024-02-25T14:30:42.684659Z","iopub.status.idle":"2024-02-25T14:59:31.753567Z","shell.execute_reply.started":"2024-02-25T14:30:42.684625Z","shell.execute_reply":"2024-02-25T14:59:31.752650Z"},"trusted":true},"execution_count":15,"outputs":[{"name":"stderr","text":"100%|██████████| 20000/20000 [28:49<00:00, 11.57it/s]\n","output_type":"stream"}]},{"cell_type":"code","source":"actual_labels = []\nwith torch.no_grad():\n    for batch in tqdm(test_dataloader):\n        labels = batch['labels'].to(device)  # Assuming labels are on the same device\n        actual_labels.extend(labels.cpu().numpy())","metadata":{"execution":{"iopub.status.busy":"2024-02-25T15:00:05.057674Z","iopub.execute_input":"2024-02-25T15:00:05.058336Z","iopub.status.idle":"2024-02-25T15:00:22.257489Z","shell.execute_reply.started":"2024-02-25T15:00:05.058297Z","shell.execute_reply":"2024-02-25T15:00:22.256346Z"},"trusted":true},"execution_count":16,"outputs":[{"name":"stderr","text":"100%|██████████| 20000/20000 [00:17<00:00, 1163.52it/s]\n","output_type":"stream"}]},{"cell_type":"code","source":"correct_predictions = sum(p == a for p, a in zip(predictions, actual_labels))\naccuracy = correct_predictions / len(predictions)\nprint(f\"Accuracy: {accuracy * 100:.2f}%\")\n","metadata":{"execution":{"iopub.status.busy":"2024-02-25T15:00:55.892029Z","iopub.execute_input":"2024-02-25T15:00:55.892421Z","iopub.status.idle":"2024-02-25T15:00:55.910573Z","shell.execute_reply.started":"2024-02-25T15:00:55.892390Z","shell.execute_reply":"2024-02-25T15:00:55.909463Z"},"trusted":true},"execution_count":17,"outputs":[{"name":"stdout","text":"Accuracy: 89.92%\n","output_type":"stream"}]},{"cell_type":"code","source":"from sklearn.metrics import precision_score, recall_score, f1_score, roc_auc_score, confusion_matrix, matthews_corrcoef\n\n# Ensure predictions and actual_labels are numpy arrays or compatible formats\nprecision = precision_score(actual_labels, predictions)\nrecall = recall_score(actual_labels, predictions)\nf1 = f1_score(actual_labels, predictions)\nconf_matrix = confusion_matrix(actual_labels, predictions)\nmcc = matthews_corrcoef(actual_labels, predictions)\n\n# ROC-AUC score requires probability scores of the positive class, which might need model.predict_proba() or equivalent\n# If your model outputs probabilities, you can use:\n# roc_auc = roc_auc_score(actual_labels, prediction_probabilities)\n\nprint(f\"Precision: {precision:.2f}\")\nprint(f\"Recall: {recall:.2f}\")\nprint(f\"F1 Score: {f1:.2f}\")\nprint(f\"Confusion Matrix:\\n{conf_matrix}\")\nprint(f\"Matthews Correlation Coefficient: {mcc:.2f}\")\n# print(f\"ROC-AUC Score: {roc_auc:.2f}\")  # Uncomment if you have probability predictions\n","metadata":{"execution":{"iopub.status.busy":"2024-02-25T15:01:11.553022Z","iopub.execute_input":"2024-02-25T15:01:11.553853Z","iopub.status.idle":"2024-02-25T15:01:11.659386Z","shell.execute_reply.started":"2024-02-25T15:01:11.553820Z","shell.execute_reply":"2024-02-25T15:01:11.658283Z"},"trusted":true},"execution_count":18,"outputs":[{"name":"stdout","text":"Precision: 0.87\nRecall: 0.94\nF1 Score: 0.90\nConfusion Matrix:\n[[8503 1432]\n [ 583 9482]]\nMatthews Correlation Coefficient: 0.80\n","output_type":"stream"}]},{"cell_type":"markdown","source":"RAW MODEL","metadata":{}},{"cell_type":"code","source":"tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\", do_lower_case=True)\nraw_model = GPT2ForSequenceClassification.from_pretrained(\"gpt2\", num_labels=2)\nraw_model = raw_model.to(device=device)\nraw_model.eval()\n\n\n# Store predictions and actual labels\nraw_predictions = []\nraw_actuals = []\n\nwith torch.no_grad():\n    for batch in tqdm(test_dataloader):\n        # Move tensors to the same device as the model\n        input_ids = batch['input_ids'].to(device)\n        attention_mask = batch['attention_mask'].to(device)\n        labels = batch['labels'].to(device)  # Only needed if you're also evaluating performance\n\n        outputs = raw_model(input_ids, attention_mask=attention_mask)\n\n        # Assuming you're doing classification and want the highest probability class\n        logits = outputs[0]\n        raw_predicted_labels = torch.argmax(logits, dim=1)\n        raw_predictions.extend(raw_predicted_labels.cpu().numpy())","metadata":{"execution":{"iopub.status.busy":"2024-02-25T15:42:03.466309Z","iopub.execute_input":"2024-02-25T15:42:03.466650Z"},"trusted":true},"execution_count":null,"outputs":[{"name":"stderr","text":"Some weights of GPT2ForSequenceClassification were not initialized from the model checkpoint at gpt2 and are newly initialized: ['score.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n 18%|█▊        | 3685/20000 [05:17<23:31, 11.56it/s]","output_type":"stream"}]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}