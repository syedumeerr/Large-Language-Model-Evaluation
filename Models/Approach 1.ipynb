{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "%cd /content/drive/MyDrive/IMDBText\n",
        "\n",
        "import pandas as pd\n",
        "imdb_train_data = pd.read_csv('train.csv')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WKfq_QLOj8xG",
        "outputId": "8dbb6797-dc1c-49b2-f56e-40493e4462d6"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "/content/drive/MyDrive/IMDBText\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install afinn"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xIHAAlzKYoy_",
        "outputId": "7bbe04d7-b39b-4933-9f4c-4f01ebb0b3d9"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting afinn\n",
            "  Downloading afinn-0.1.tar.gz (52 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/52.6 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m52.6/52.6 kB\u001b[0m \u001b[31m3.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Building wheels for collected packages: afinn\n",
            "  Building wheel for afinn (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for afinn: filename=afinn-0.1-py3-none-any.whl size=53429 sha256=abe43e22fe830c4382f4f0573f7853ff1521addd8a34d5bfe1b4abd9eb5dfae5\n",
            "  Stored in directory: /root/.cache/pip/wheels/b0/05/90/43f79196199a138fb486902fceca30a2d1b5228e6d2db8eb90\n",
            "Successfully built afinn\n",
            "Installing collected packages: afinn\n",
            "Successfully installed afinn-0.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "nltk.download('punkt')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('sentiwordnet')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P610fUc-u50I",
        "outputId": "5f095949-d758-4f16-9193-c460f41d6114"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
            "[nltk_data]       date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package sentiwordnet to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/sentiwordnet.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import time\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk.corpus import sentiwordnet as swn\n",
        "from nltk import pos_tag\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score\n",
        "from nltk.corpus import wordnet\n",
        "from afinn import Afinn\n",
        "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
        "from nltk.corpus import stopwords\n",
        "import string\n",
        "import re"
      ],
      "metadata": {
        "id": "RFOGo3Q9sJkQ"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Pre-Processing Text"
      ],
      "metadata": {
        "id": "t70Gz1azHd1s"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Function for text preprocessing\n",
        "def preprocess_text(text):\n",
        "    # Tokenization\n",
        "    tokens = word_tokenize(text)\n",
        "    # Lemmatization\n",
        "    lemmatizer = WordNetLemmatizer()\n",
        "    lemmatized_tokens = [lemmatizer.lemmatize(token) for token in tokens]\n",
        "    # Remove stopwords and punctuation\n",
        "    stop_words = set(stopwords.words('english'))\n",
        "    filtered_tokens = [token for token in lemmatized_tokens if token.lower() not in stop_words and token not in string.punctuation]\n",
        "    # Join tokens back into text\n",
        "    preprocessed_text = ' '.join(filtered_tokens)\n",
        "    return preprocessed_text\n",
        "\n"
      ],
      "metadata": {
        "id": "hTmbTj_Vwnnf"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to get wordnet POS tag from Penn Treebank POS tag\n",
        "def get_wordnet_pos(treebank_tag):\n",
        "    if treebank_tag.startswith('J'):\n",
        "        return 'a'  # Adjective\n",
        "    elif treebank_tag.startswith('V'):\n",
        "        return 'v'  # Verb\n",
        "    elif treebank_tag.startswith('N'):\n",
        "        return 'n'  # Noun\n",
        "    elif treebank_tag.startswith('R'):\n",
        "        return 'r'  # Adverb\n",
        "    else:\n",
        "        return None"
      ],
      "metadata": {
        "id": "YrfdHNE0HuiR"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Function to get sentiment score using SentiWordNet\n",
        "def get_sentiment_score(word, pos_tag):\n",
        "    if pos_tag:\n",
        "        synsets = list(swn.senti_synsets(word, pos_tag))\n",
        "        if synsets:\n",
        "            # Take the average of positive and negative scores\n",
        "            return (synsets[0].pos_score() - synsets[0].neg_score())\n",
        "    return 0"
      ],
      "metadata": {
        "id": "nqtvU97NHwJq"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#SentiWordNet"
      ],
      "metadata": {
        "id": "fdhnaoXQHqsE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Preprocess the text data\n",
        "imdb_train_data['review'] = imdb_train_data['review'].apply(preprocess_text)\n"
      ],
      "metadata": {
        "id": "E_oLr20nw4H8"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Tokenize and tag parts of speech for each review\n",
        "imdb_train_data['tokens'] = imdb_train_data['review'].apply(word_tokenize)\n",
        "imdb_train_data['pos_tags'] = imdb_train_data['tokens'].apply(pos_tag)\n",
        "imdb_train_data['pos_tags'] = imdb_train_data['pos_tags'].apply(lambda x: [(word, get_wordnet_pos(tag)) for (word, tag) in x])\n"
      ],
      "metadata": {
        "id": "UOx6rOZ2z53M"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Start time\n",
        "start_time = time.time()\n",
        "\n",
        "# Calculate sentiment score for each review using SentiWordNet\n",
        "\n",
        "imdb_train_data['sentiment_score'] = imdb_train_data['pos_tags'].apply(lambda x: sum(get_sentiment_score(word, pos_tag) for word, pos_tag in x))\n",
        "\n",
        "# Convert sentiment score to binary sentiment label\n",
        "imdb_train_data['predicted_sentiment'] = imdb_train_data['sentiment_score'].apply(lambda x: 'positive' if x > 0 else 'negative' if x < 0 else 'neutral')\n",
        "\n",
        "# End time\n",
        "end_time = time.time()\n",
        "\n",
        "# Calculate computational time\n",
        "training_time = end_time - start_time\n"
      ],
      "metadata": {
        "id": "VtyY4cqyz9U8"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load IMDb test data\n",
        "imdb_test_data = pd.read_csv('test.csv')\n"
      ],
      "metadata": {
        "id": "hB5Gz247zJ_F"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Preprocess the test data\n",
        "imdb_test_data['review'] = imdb_test_data['review'].apply(preprocess_text)\n",
        "\n"
      ],
      "metadata": {
        "id": "C0vroj6Z-p-l"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Tokenize and tag parts of speech for each review in the test data\n",
        "imdb_test_data['tokens'] = imdb_test_data['review'].apply(word_tokenize)\n",
        "imdb_test_data['pos_tags'] = imdb_test_data['tokens'].apply(pos_tag)\n",
        "imdb_test_data['pos_tags'] = imdb_test_data['pos_tags'].apply(lambda x: [(word, get_wordnet_pos(tag)) for (word, tag) in x])\n",
        "\n"
      ],
      "metadata": {
        "id": "Oz6NryAC-rhm"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Start time\n",
        "start_time = time.time()\n",
        "\n",
        "# Calculate sentiment score for each review in the test data using SentiWordNet\n",
        "imdb_test_data['sentiment_score'] = imdb_test_data['pos_tags'].apply(lambda x: sum(get_sentiment_score(word, pos_tag) for word, pos_tag in x))\n",
        "\n",
        "# Convert sentiment score to binary sentiment label for the test data\n",
        "imdb_test_data['predicted_sentiment'] = imdb_test_data['sentiment_score'].apply(lambda x: 'positive' if x > 0 else 'negative' if x < 0 else 'neutral')\n",
        "# End time\n",
        "end_time = time.time()\n",
        "# Calculate computational time\n",
        "testing_time = end_time - start_time\n"
      ],
      "metadata": {
        "id": "gmYl2uLq-tJM"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluate the accuracy of the SentiWordNet lexicon-based approach\n",
        "accuracy = accuracy_score(imdb_test_data['sentiment'], imdb_test_data['predicted_sentiment'])\n",
        "print(\"Accuracy of SentiWordNet lexicon-based approach on IMDb test data:\", accuracy)\n",
        "\n",
        "print(\"Train time:\", training_time, \"seconds\")\n",
        "\n",
        "print(\"test time:\", testing_time, \"seconds\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k28vz5iA_-Xe",
        "outputId": "53c8d835-ee4b-45b2-8b70-aa6545e53cd0"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy of SentiWordNet lexicon-based approach on IMDb test data: 0.63005\n",
            "Train time: 108.77739357948303 seconds\n",
            "test time: 70.90979218482971 seconds\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **Accuracy of SentiWordNet on test data: 0.63005**"
      ],
      "metadata": {
        "id": "x6zpWpIYIffS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#AFINN"
      ],
      "metadata": {
        "id": "XSZFe_sYF1_H"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from afinn import Afinn\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "\n",
        "\n",
        "# Instantiate the AFINN lexicon\n",
        "afinn = Afinn()\n",
        "# Start time\n",
        "start_time = time.time()\n",
        "# Calculate sentiment scores for each review in the test data using AFINN\n",
        "imdb_test_data['sentiment_score'] = imdb_test_data['review'].apply(afinn.score)\n",
        "\n",
        "# Convert sentiment scores to binary sentiment labels\n",
        "imdb_test_data['predicted_sentiment'] = imdb_test_data['sentiment_score'].apply(lambda score: 'positive' if score > 0 else 'negative' if score < 0 else 'neutral')\n",
        "# End time\n",
        "end_time = time.time()\n",
        "# Calculate computational time\n",
        "test_time = end_time - start_time"
      ],
      "metadata": {
        "id": "22L06dfV__hs"
      },
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluate the accuracy of the AFINN lexicon approach\n",
        "accuracy = accuracy_score(imdb_test_data['sentiment'], imdb_test_data['predicted_sentiment'])\n",
        "print(\"Accuracy of AFINN lexicon approach on IMDb test data:\", accuracy)\n",
        "print(\"test time:\", test_time, \"seconds\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dZNCRJSfaNpp",
        "outputId": "20baabd1-90a0-428b-cd4e-1b6641018072"
      },
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy of AFINN lexicon approach on IMDb test data: 0.6829\n",
            "test time: 41.70845556259155 seconds\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **Accuracy of AFIN on test data: 0.68275**"
      ],
      "metadata": {
        "id": "AN3iwQm6IWWm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Semi-Supervised Learning Using Lexicon Method"
      ],
      "metadata": {
        "id": "y8WEy0PdGxyU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculate sentiment scores for labeled data\n",
        "imdb_train_data['sentiment_score'] = imdb_train_data['review'].apply(afinn.score)\n"
      ],
      "metadata": {
        "id": "Smf-Srd3DQl-"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create TF-IDF vectors for labeled data\n",
        "tfidf_vectorizer = TfidfVectorizer(max_features=5000)\n",
        "X_train_tfidf = tfidf_vectorizer.fit_transform(imdb_train_data['review'])\n",
        "y_train = imdb_train_data['sentiment']"
      ],
      "metadata": {
        "id": "vWt7zijPIHAw"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Start time\n",
        "start_time = time.time()\n",
        "# Train a logistic regression classifier using labeled data and sentiment scores\n",
        "classifier = LogisticRegression()\n",
        "classifier.fit(X_train_tfidf, y_train)\n",
        "# End time\n",
        "end_time = time.time()\n",
        "# Calculate computational time\n",
        "training_time = end_time - start_time"
      ],
      "metadata": {
        "id": "LAa8wOxYII8b"
      },
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Preprocess the test data\n",
        "imdb_test_data['review'] = imdb_test_data['review'].apply(preprocess_text)\n",
        "\n",
        "# Create TF-IDF vectors for test data\n",
        "X_test_tfidf = tfidf_vectorizer.transform(imdb_test_data['review'])"
      ],
      "metadata": {
        "id": "5gaAKx26IMzY"
      },
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Start time\n",
        "start_time = time.time()\n",
        "# Predict sentiment labels for test data\n",
        "y_pred = classifier.predict(X_test_tfidf)\n",
        "# End time\n",
        "end_time = time.time()\n",
        "# Calculate computational time\n",
        "test_time = end_time - start_time\n"
      ],
      "metadata": {
        "id": "PzOxHTvdIPtl"
      },
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluate the accuracy of the classifier on test data\n",
        "accuracy = accuracy_score(imdb_test_data['sentiment'], y_pred)\n",
        "print(\"Accuracy of semi-supervised learning using lexicon-based methods on IMDb test data:\", accuracy)\n",
        "print(\"Train time:\", training_time, \"seconds\")\n",
        "print(\"test time:\", test_time, \"seconds\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0KYKKOjycD-s",
        "outputId": "07fcfb7b-a61c-4509-ec8e-db89eba987b1"
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy of semi-supervised learning using lexicon-based methods on IMDb test data: 0.88685\n",
            "Train time: 0.536574125289917 seconds\n",
            "test time: 0.004080295562744141 seconds\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **Accuracy of semi-supervised learning using lexicon on test data: 0.88685**"
      ],
      "metadata": {
        "id": "tIUHrx5Ri4Ka"
      }
    }
  ]
}